# Lecture-7: Generative Deep Learning

In this lecture, we will:

### Deep Dive into VAEs:
- Understand how **Variational Autoencoders (VAEs)** learn a probabilistic latent space.
- Explore the **reparameterization trick**, KL divergence, and reconstruction loss.
- Visualize the **latent space** of a VAE trained on the **MNIST** dataset.
- Discuss real-world **applications** of autoencoders, including:
  - **Gravitational wave denoising** using deep autoencoders.

Notebook: `Autoencoders.ipynb`  
Notebook: `VAEs.ipynb`   
Notebook: `Autoencoder_GW_Denoising.ipynb`  


### Brief Introduction to GANs:
- Get a high-level overview of **Generative Adversarial Networks (GANs)**.
- Understand the generatorâ€“discriminator game and the idea of adversarial training.
- Train a simple GAN on the **FASHION-MNIST** dataset and explore generated samples.

Notebook: `GANs.ipynb`

By the end of this lecture, you will:
- Understand the core ideas behind VAEs and how they differ from traditional autoencoders.
- See how latent space representations can be visualized and interpreted.
- Be introduced to GANs as a powerful alternative for data generation.
