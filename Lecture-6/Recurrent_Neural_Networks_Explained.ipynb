{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496e1181",
   "metadata": {},
   "source": [
    "## Vanilla RNN Cell Explained\n",
    "\n",
    "A Recurrent Neural Network (RNN) processes sequences by maintaining a **hidden state** that is updated at each time step based on the current input and the previous hidden state.\n",
    "\n",
    "### Update Rule\n",
    "\n",
    "At each time step $t$, the hidden state $h_t$ is updated using:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{ih} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_t \\in \\mathbb{R}^{n}$: input at time step $t$\n",
    "- $h_{t-1} \\in \\mathbb{R}^{m}$: previous hidden state\n",
    "- $W_{ih} \\in \\mathbb{R}^{m \\times n}$: input-to-hidden weight matrix\n",
    "- $W_{hh} \\in \\mathbb{R}^{m \\times m}$: hidden-to-hidden weight matrix\n",
    "- $b \\in \\mathbb{R}^{m}$: bias vector\n",
    "- $\\tanh(\\cdot)$: element-wise hyperbolic tangent activation\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- The hidden state $h_t$ captures information from **all previous inputs** $x_0, x_1, ..., x_t$.\n",
    "- The weights $W_{ih}$ and $W_{hh}$ are **shared** across all time steps.\n",
    "- This allows the RNN to process variable-length sequences using a fixed number of parameters.\n",
    "\n",
    "### Limitation\n",
    "\n",
    "RNNs are prone to the **vanishing gradient problem**, especially for long sequences. This occurs because the gradient of the loss with respect to earlier hidden states involves a product of many small Jacobian terms, which can shrink towards zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_t} = \\frac{\\partial \\mathcal{L}}{\\partial h_T} \\cdot \\prod_{k=t+1}^{T} \\frac{\\partial h_k}{\\partial h_{k-1}}\n",
    "$$\n",
    "\n",
    "As a result, it becomes difficult to learn long-term dependencies.\n",
    "\n",
    "You can test a simple RNN implementation using raw PyTorch operations to observe how the hidden state evolves over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa90c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN outputs: [0.15289130806922913, 0.294706791639328, 0.4248957335948944, 0.5398406982421875, 0.6379193663597107]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Input sequence: shape (seq_len, input_size)\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])  # (5, 1)\n",
    "\n",
    "# Dimensions\n",
    "seq_len, input_size = x.shape\n",
    "hidden_size = 1\n",
    "\n",
    "# Initialize weights and bias\n",
    "W_ih = torch.randn(hidden_size, input_size) * 0.1  # input → hidden\n",
    "W_hh = torch.randn(hidden_size, hidden_size) * 0.1  # hidden → hidden\n",
    "b = torch.zeros(hidden_size)                       # bias\n",
    "\n",
    "# Output list\n",
    "outputs = []\n",
    "\n",
    "# Initial hidden state\n",
    "h = torch.zeros(hidden_size)\n",
    "# RNN forward loop\n",
    "for t in range(seq_len):\n",
    "    x_t = x[t]\n",
    "    h = torch.tanh(W_ih @ x_t + W_hh @ h + b)\n",
    "    outputs.append(h.item())\n",
    "\n",
    "# Print output hidden states\n",
    "print(\"RNN outputs:\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaaff60",
   "metadata": {},
   "source": [
    "##  Vanilla RNN Using PyTorch\n",
    "\n",
    "PyTorch provides a built-in implementation of vanilla RNNs through `nn.RNN` or `nn.RNNCell` (for a single cell). It handles weight initialization, recurrence, and backpropagation through time (BPTT) internally.\n",
    "\n",
    "### PyTorch Structure\n",
    "\n",
    "- **Input shape**: `(seq_len, batch_size, input_size)`\n",
    "- **Output**:\n",
    "  - `output`: all hidden states $(h_0, h_1, ..., h_T)$ → shape `(seq_len, batch_size, hidden_size)`\n",
    "  - `h_n`: final hidden state $h_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68edf2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All hidden states:\n",
      " tensor([[-0.7895, -0.0327, -0.3268, -0.0262],\n",
      "        [-0.9114, -0.2199, -0.6464, -0.3298],\n",
      "        [-0.9641, -0.3921, -0.8333, -0.5777],\n",
      "        [-0.9857, -0.5407, -0.9259, -0.7511],\n",
      "        [-0.9943, -0.6618, -0.9679, -0.8596]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define input parameters\n",
    "input_size = 1      # size of each input vector\n",
    "hidden_size = 4     # number of hidden units\n",
    "seq_len = 5         # number of time steps\n",
    "\n",
    "# Input sequence: shape (seq_len, input_size)\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])  # shape (5, 1)\n",
    "\n",
    "# Define the RNN module\n",
    "rnn = nn.RNNCell(input_size=input_size, hidden_size=hidden_size, nonlinearity='tanh')\n",
    "\n",
    "# Initial hidden state (optional)\n",
    "h0 = torch.zeros(5, hidden_size)\n",
    "\n",
    "# Run the RNN\n",
    "output = rnn(x, h0)\n",
    "\n",
    "# Print outputs\n",
    "print(\"All hidden states:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3535c3f",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU) Explained\n",
    "\n",
    "A **GRU** is a type of recurrent neural network that introduces **gates** to control the flow of information, helping it retain memory over longer sequences and mitigate the vanishing gradient problem.\n",
    "\n",
    "### GRU Equations\n",
    "\n",
    "At each time step $t$, given input $x$ and previous hidden state $h$, the GRU computes:\n",
    "\n",
    "#### 1. **Reset Gate** (controls how much past to forget)\n",
    "\n",
    "$$\n",
    "t_{hr} = W_{hr} h + b_{hr} \\\\\n",
    "t_{xr} = W_{ir} x + b_{ir} \\\\\n",
    "r = \\sigma(t_{hr} + t_{xr})\n",
    "$$\n",
    "\n",
    "#### 2. **Update Gate** (controls how much new info to use)\n",
    "\n",
    "$$\n",
    "t_{hz} = W_{hz} h + b_{hz} \\\\\n",
    "t_{xz} = W_{iz} x + b_{iz} \\\\\n",
    "z = \\sigma(t_{hz} + t_{xz})\n",
    "$$\n",
    "\n",
    "#### 3. **Candidate Activation** (proposed new hidden state)\n",
    "\n",
    "$$\n",
    "t_{hn} = W_{hn} h + b_{hn} \\\\\n",
    "t_{xn} = W_{in} x + b_{in} \\\\\n",
    "\\tilde{h} = \\tanh(t_{xn} + r \\odot t_{hn})\n",
    "$$\n",
    "\n",
    "#### 4. **Final Hidden State Update**\n",
    "\n",
    "$$\n",
    "h_t = (1 - z) \\odot h + z \\odot \\tilde{h}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\sigma(\\cdot)$ is the sigmoid function\n",
    "- $\\tanh(\\cdot)$ is the hyperbolic tangent\n",
    "- $\\odot$ denotes element-wise multiplication\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- The **reset gate** $r$ decides how much of the past state to forget when computing the candidate $\\tilde{h}$.\n",
    "- The **update gate** $z$ decides how much of the new candidate to use vs. keeping the old state.\n",
    "- This gating mechanism enables the GRU to **learn long-range dependencies** and avoid gradient vanishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f15075c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU outputs: [-0.05656343698501587, -0.14032450318336487, -0.2349534034729004, -0.3311828374862671, -0.42367157340049744]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Input sequence: shape (seq_len, input_size)\n",
    "sequence = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])  # shape (5, 1)\n",
    "seq_len, input_size = sequence.shape\n",
    "hidden_size = 1  # scalar GRU cell\n",
    "\n",
    "# Initialize GRU weights and biases\n",
    "W_ir = torch.randn(hidden_size, input_size) * 0.1\n",
    "W_hr = torch.randn(hidden_size, hidden_size) * 0.1\n",
    "b_ir = torch.zeros(hidden_size)\n",
    "b_hr = torch.zeros(hidden_size)\n",
    "\n",
    "W_iz = torch.randn(hidden_size, input_size) * 0.1\n",
    "W_hz = torch.randn(hidden_size, hidden_size) * 0.1\n",
    "b_iz = torch.zeros(hidden_size)\n",
    "b_hz = torch.zeros(hidden_size)\n",
    "\n",
    "W_in = torch.randn(hidden_size, input_size) * 0.1\n",
    "W_hn = torch.randn(hidden_size, hidden_size) * 0.1\n",
    "b_in = torch.zeros(hidden_size)\n",
    "b_hn = torch.zeros(hidden_size)\n",
    "\n",
    "# Initial hidden state\n",
    "h = torch.zeros(hidden_size)\n",
    "\n",
    "# GRU forward pass\n",
    "outputs = []\n",
    "for t in range(seq_len):\n",
    "    x_t = sequence[t]\n",
    "\n",
    "    # Reset gate\n",
    "    r = torch.sigmoid(W_ir @ x_t + b_ir + W_hr @ h + b_hr)\n",
    "\n",
    "    # Update gate\n",
    "    z = torch.sigmoid(W_iz @ x_t + b_iz + W_hz @ h + b_hz)\n",
    "\n",
    "    # Candidate hidden state\n",
    "    n = torch.tanh(W_in @ x_t + b_in + r * (W_hn @ h + b_hn))\n",
    "\n",
    "    # Final hidden state\n",
    "    h = (1 - z) * h + z * n\n",
    "    outputs.append(h.item())\n",
    "\n",
    "print(\"GRU outputs:\", outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e3cb1",
   "metadata": {},
   "source": [
    "## GRU (Gated Recurrent Unit) Using PyTorch\n",
    "\n",
    "PyTorch provides a built-in GRU module via `nn.GRU` or `nn.GRUCell` (for single cell), which simplifies training and inference for recurrent networks with gating mechanisms.\n",
    "\n",
    "### PyTorch GRU Structure\n",
    "\n",
    "- **Input shape**: `(seq_len, batch_size, input_size)`\n",
    "- **Output**:\n",
    "  - `output`: all hidden states $(h_0, ..., h_T)$ → shape `(seq_len, batch_size, hidden_size)`\n",
    "  - `h_n`: final hidden state $h_T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e374a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All hidden states (output): tensor([[-0.4216, -0.0231, -0.2176, -0.2058],\n",
      "        [-0.5988,  0.0482, -0.4087, -0.3461],\n",
      "        [-0.7258,  0.1265, -0.5571, -0.4507],\n",
      "        [-0.8130,  0.2086, -0.6629, -0.5267],\n",
      "        [-0.8723,  0.2918, -0.7372, -0.5849]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters\n",
    "input_size = 1\n",
    "hidden_size = 4\n",
    "seq_len = 5\n",
    "\n",
    "# Input sequence: shape (seq_len, input_size)\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])  # shape (5, 1)\n",
    "\n",
    "# Define GRU model\n",
    "gru = nn.GRUCell(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "# Initial hidden state (optional)\n",
    "h0 = torch.zeros(5, hidden_size)\n",
    "\n",
    "# Forward pass\n",
    "output = gru(x, h0)\n",
    "\n",
    "print(\"All hidden states (output):\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7818e5",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM) in PyTorch\n",
    "\n",
    "An **LSTM** is a gated recurrent neural network designed to capture **long-range dependencies** and mitigate the **vanishing gradient problem** by introducing a **cell state** that is updated additively.\n",
    "\n",
    "### LSTM Equations\n",
    "\n",
    "At each time step $t$, the LSTM takes input $x_t$ and previous states $(h_{t-1}, c_{t-1})$ and computes:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_t &= \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\quad \\text{(forget gate)} \\\\\n",
    "i_t &= \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\quad \\text{(input gate)} \\\\\n",
    "o_t &= \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\quad \\text{(output gate)} \\\\\n",
    "\\tilde{c}_t &= \\tanh(W_{ic} x_t + b_{ic} + W_{hc} h_{t-1} + b_{hc}) \\quad \\text{(cell candidate)} \\\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\quad \\text{(new cell state)} \\\\\n",
    "h_t &= o_t \\odot \\tanh(c_t) \\quad \\text{(new hidden state)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $\\sigma(\\cdot)$ is the sigmoid function\n",
    "- $\\tanh(\\cdot)$ is the hyperbolic tangent\n",
    "- $\\odot$ is element-wise multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08fb46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All hidden states (output): tensor([[ 0.0153,  0.0893, -0.1494, -0.1791],\n",
      "        [-0.0168,  0.0562, -0.3511, -0.3371],\n",
      "        [-0.0600, -0.0069, -0.5769, -0.4640],\n",
      "        [-0.0847, -0.0451, -0.7566, -0.5572],\n",
      "        [-0.0849, -0.0594, -0.8628, -0.6209]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters\n",
    "input_size = 1\n",
    "hidden_size = 4\n",
    "seq_len = 5\n",
    "\n",
    "# Input sequence: shape (seq_len, input_size)\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])  # shape (5, 1)\n",
    "\n",
    "# LSTM cell\n",
    "lstm = nn.LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "# Initial hidden and cell states\n",
    "h_t = torch.zeros(1, hidden_size)\n",
    "c_t = torch.zeros(1, hidden_size)\n",
    "\n",
    "# Forward loop\n",
    "outputs = []\n",
    "for t in range(seq_len):\n",
    "    h_t, c_t = lstm(x[t].unsqueeze(0), (h_t, c_t))  # (1, hidden_size)\n",
    "    outputs.append(h_t)\n",
    "\n",
    "# Stack outputs to a tensor: shape (seq_len, hidden_size)\n",
    "outputs = torch.cat(outputs, dim=0)\n",
    "\n",
    "print(\"All hidden states (output):\", outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
